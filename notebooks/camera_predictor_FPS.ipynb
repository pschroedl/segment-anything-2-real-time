{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from IPython import display\n",
    "import time\n",
    "from sam2.build_sam import build_sam2_camera_predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use bfloat16 for the entire notebook\n",
    "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {},
   "source": [
    "### Building the SAM 2 camera predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam2_checkpoint = \"../checkpoints/sam2_hiera_tiny.pt\"\n",
    "model_cfg = \"sam2_hiera_t.yaml\"\n",
    "\n",
    "predictor = build_sam2_camera_predictor(model_cfg, sam2_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64f966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels == 1]\n",
    "    neg_points = coords[labels == 0]\n",
    "    ax.scatter(\n",
    "        pos_points[:, 0],\n",
    "        pos_points[:, 1],\n",
    "        color=\"green\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        neg_points[:, 0],\n",
    "        neg_points[:, 1],\n",
    "        color=\"red\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "\n",
    "\n",
    "def show_bbox(bbox, ax, marker_size=200):\n",
    "    tl, br = bbox[0], bbox[1]\n",
    "    w, h = (br - tl)[0], (br - tl)[1]\n",
    "    x, y = tl[0], tl[1]\n",
    "    print(x, y, w, h)\n",
    "    ax.add_patch(plt.Rectangle((x, y), w, h, fill=None, edgecolor=\"blue\", linewidth=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad",
   "metadata": {},
   "source": [
    "#### Select an video stream (video or camera)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23bd0bd-8011-4e3b-868d-732fb12bc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import HTML\n",
    "\n",
    "# # Replace with your public IP address\n",
    "# video_stream_url = \"http://66.27.122.32:5050/video_feed?key=12903hjk1230\"\n",
    "\n",
    "# HTML(f\"\"\"\n",
    "# <iframe src=\"{video_stream_url}\" width=\"640\" height=\"480\"></iframe>\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b94c87ca-fd1a-4011-9609-e8be1cbe3230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58eecd3b-c32e-463b-b43a-585b8076c9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 333 frames in 15.44 seconds (FPS: 21.56)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"../webcam_test.mp4\")\n",
    "ret, frame = cap.read()\n",
    "width, height = frame.shape[:2][::-1]\n",
    "\n",
    "if_init = False\n",
    "frame_count = 0  # Frame counter for FPS calculation\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        height, width = frame.shape[:2]\n",
    "        frame_count += 1  # Increment frame count\n",
    "\n",
    "        # Only perform initialization on the first frame\n",
    "        if not if_init:\n",
    "            predictor.load_first_frame(frame)\n",
    "            if_init = True\n",
    "            obj_id = 1  # Example object ID\n",
    "            frame_idx = 0\n",
    "\n",
    "            # Define the point prompt at one-third from the right, centered vertically\n",
    "            point = [int(width * 2 / 3), int(height / 2)]\n",
    "            points = [point]\n",
    "            labels = [1]  # Positive prompt\n",
    "\n",
    "            # Initialize segmentation with the point prompt\n",
    "            _, out_obj_ids, out_mask_logits = predictor.add_new_prompt(frame_idx, obj_id, points=points, labels=labels)\n",
    "\n",
    "        else:\n",
    "            # Track the object in subsequent frames\n",
    "            out_obj_ids, out_mask_logits = predictor.track(frame)\n",
    "            \n",
    "        # Process output mask only if it's non-empty\n",
    "        if out_mask_logits.shape[0] > 0:\n",
    "            mask = (out_mask_logits[0, 0] > 0).cpu().numpy().astype(\"uint8\") * 255\n",
    "        else:\n",
    "            mask = np.zeros((height, width), dtype=\"uint8\")\n",
    "\n",
    "        # Invert and prepare the mask for overlay (not displayed)\n",
    "        inverted_mask_colored = cv2.cvtColor(cv2.bitwise_not(mask), cv2.COLOR_GRAY2BGR)\n",
    "        overlayed_frame = cv2.addWeighted(frame, 0.7, inverted_mask_colored, 0.3, 0)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "cap.release()\n",
    "\n",
    "# Calculate FPS\n",
    "total_time = end_time - start_time\n",
    "fps = frame_count / total_time\n",
    "print(f\"Processed {frame_count} frames in {total_time:.2f} seconds (FPS: {fps:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac6a63-d102-4654-b539-6a92f448680d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5001\n",
      " * Running on http://204.12.253.6:5001\n",
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5001\n",
      " * Running on http://204.12.253.6:5001\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from flask import Flask, Response\n",
    "import threading\n",
    "\n",
    "# URL for accessing the raw webcam feed from the local machine\n",
    "video_feed_url = \"http://66.27.122.32:5050/video_feed?key=12903hjk1230\"\n",
    "\n",
    "# Flask app to serve the processed video stream\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Frame lock and processed frame storage\n",
    "frame_lock = threading.Lock()\n",
    "processed_frame = None\n",
    "\n",
    "# Function to capture and process the frames from the raw feed\n",
    "def process_frames():\n",
    "    global processed_frame\n",
    "    cap = cv2.VideoCapture(video_feed_url)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Unable to open video feed from URL.\")\n",
    "\n",
    "    if_init = False\n",
    "    frame_count = 0  # Frame counter for FPS calculation\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            height, width = frame.shape[:2]\n",
    "            frame_count += 1  # Increment frame count\n",
    "\n",
    "            # Only perform initialization on the first frame\n",
    "            if not if_init:\n",
    "                predictor.load_first_frame(frame)\n",
    "                if_init = True\n",
    "                obj_id = 1  # Example object ID\n",
    "                frame_idx = 0\n",
    "\n",
    "                # Define the point prompt at one-third from the right, centered vertically\n",
    "                point = [int(width * 2 / 3), int(height / 2)]\n",
    "                points = [point]\n",
    "                labels = [1]  # Positive prompt\n",
    "\n",
    "                # Initialize segmentation with the point prompt\n",
    "                _, out_obj_ids, out_mask_logits = predictor.add_new_prompt(frame_idx, obj_id, points=points, labels=labels)\n",
    "            else:\n",
    "                # Track the object in subsequent frames\n",
    "                out_obj_ids, out_mask_logits = predictor.track(frame)\n",
    "\n",
    "            # Process output mask only if it's non-empty\n",
    "            if out_mask_logits.shape[0] > 0:\n",
    "                mask = (out_mask_logits[0, 0] > 0).cpu().numpy().astype(\"uint8\") * 255\n",
    "            else:\n",
    "                mask = np.zeros((height, width), dtype=\"uint8\")\n",
    "\n",
    "            # Invert and prepare the mask for overlay (not displayed)\n",
    "            inverted_mask_colored = cv2.cvtColor(cv2.bitwise_not(mask), cv2.COLOR_GRAY2BGR)\n",
    "            overlayed_frame = cv2.addWeighted(frame, 0.7, inverted_mask_colored, 0.3, 0)\n",
    "\n",
    "            # Update the processed frame for Flask\n",
    "            with frame_lock:\n",
    "                processed_frame = overlayed_frame\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    cap.release()\n",
    "\n",
    "    # Calculate FPS\n",
    "    total_time = end_time - start_time\n",
    "    fps = frame_count / total_time\n",
    "    print(f\"Processed {frame_count} frames in {total_time:.2f} seconds (FPS: {fps:.2f})\")\n",
    "\n",
    "# Start a background thread to process the frames continuously\n",
    "processing_thread = threading.Thread(target=process_frames)\n",
    "processing_thread.daemon = True\n",
    "processing_thread.start()\n",
    "\n",
    "# Flask endpoint to stream the processed video\n",
    "@app.route('/processed_feed')\n",
    "def processed_feed():\n",
    "    def generate_processed_frames():\n",
    "        global processed_frame\n",
    "        while True:\n",
    "            with frame_lock:\n",
    "                if processed_frame is None:\n",
    "                    continue\n",
    "                _, buffer = cv2.imencode('.jpg', processed_frame)\n",
    "                frame = buffer.tobytes()\n",
    "                yield (b'--frame\\r\\n'\n",
    "                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n",
    "\n",
    "    return Response(generate_processed_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=5001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd11c5-5826-445f-b354-fdfae7d45e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# URL for the processed video feed served by the Flask app in the remote Jupyter notebook\n",
    "processed_feed_url = \"http://0.0.0.0:5001/processed_feed\"\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<iframe src=\"{processed_feed_url}\" width=\"640\" height=\"480\"></iframe>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2854db3a-a029-468f-aaad-2a2ef3a993b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
